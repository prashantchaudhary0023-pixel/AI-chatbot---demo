{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aaba3ed-327e-4e91-8054-e1119d44498d",
   "metadata": {},
   "source": [
    "# CHATBOT AGENT using LANGCHAIN and OPENAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8332dde-caca-4b83-826d-f5fb5d0439c1",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47aa59aa-aef5-40a9-8d9c-500777c32e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install openai==1.55.2\n",
    "#!python3 -m pip install langchain==0.3.9 --user \n",
    "#! pip install langchain-openai==0.2.10\n",
    "#! pip install -U langchain-pinecone==0.2.0 \n",
    "#(Some older versions are used due to compatibility issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a85af-a20f-4997-9f43-37c28c48266f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25c44ca-68dd-4368-b266-2e22b15edcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create openai key and pinecone key\n",
    "# Assign the API key as a string to a variable\n",
    "OPENAI_API_KEY =\"Your_openai_secret_key\"\n",
    "MY_KEY= \"Your_pinecone_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2857c286-6a2f-417f-8919-ecc0456fdcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rajpurkar.github.io/SQuAD-explorer/\n",
    "# we will be using squad 2.0 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3bece9d-1493-4766-b03f-39129f0cafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset by importing load_dataset\n",
    "from datasets import load_dataset \n",
    "data = load_dataset('squad', split='train') # using the training data\n",
    "\n",
    "# convert this dataset to pandas dataframe\n",
    "import pandas as pd\n",
    "df = data.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5345d1fa-1605-4d3d-b6fd-26b176287f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the context is duplicated, we need to clean and remove duplicates\n",
    "df.drop_duplicates(subset='context', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e4512-c23e-424b-9b19-0d46aa0c00c1",
   "metadata": {},
   "source": [
    "# Embedding via API\n",
    "## convert documents to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "657ac6c2-9ea3-4ec6-9809-01b93b0b6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use openai for embedding and create a helper function\n",
    "# so that we dont have to embed every text individually\n",
    "\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "#This model is used as it takes lesser credits compared to other text converters\n",
    "\n",
    "# Initiate a client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", OPENAI_API_KEY))\n",
    "\n",
    "# helper function for embeddings\n",
    "def get_embedding(text, model):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    res = client.embeddings.create(input = text, model = model) \n",
    "    return (res.data)[0].embedding\n",
    "# check the dimension of embedded vector as the same dimension will be used\n",
    "# for Pinecone index creation\n",
    "# here dimension=1536\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49fb095f-a902-4bd8-bbe5-aba607fe8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use langchain for embeddings\n",
    "# we will use this for better compatibilty between openai and langchain\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "embed = OpenAIEmbeddings(\n",
    "    model = MODEL,\n",
    "    openai_api_key= OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e190a-e8ad-4dc0-8d9f-f30a932fbaef",
   "metadata": {},
   "source": [
    "# Vector DB setup\n",
    "## Pinecone setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3730d9bf-4b84-489f-9b47-972a8253500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "# MY_KEY = \"YOUR API KEY\"\n",
    "pc = Pinecone(api_key = MY_KEY)\n",
    "\n",
    "# create an index\n",
    "pc.create_index(\"ai-agent\", dimension=1536, metric='dotproduct',\n",
    "                     spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "# dimensions of 1536\n",
    "# region=\"us-east-1\" for free usage\n",
    "\n",
    "# Initiate the index\n",
    "index = pc.Index(\"ai-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8955f9-b1fc-48e7-8923-6f2744812222",
   "metadata": {},
   "source": [
    "# Indexing/Upserting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "339fb35b-f5c0-49c2-a9d6-ab7db57b6e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30220d3b7424e56b2dfa25f7601ae7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a df sample due to rate limit on openai\n",
    "\n",
    "df_sample = df.sample(1000, random_state=45)\n",
    "batch_size = 100\n",
    "\n",
    "# prepare ids,embeddings,metadata for upserting\n",
    "from tqdm.auto import tqdm #for large datasets\n",
    "import time #optional\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, len(df_sample), batch_size)):\n",
    "    i_end = min(i+batch_size, len(df_sample))\n",
    "#     print(i, i_end)\n",
    "    batch = df_sample.iloc[i:i_end]\n",
    "    meta_data = [{\"title\" : row['title'], \n",
    "              \"context\": row['context']} \n",
    "             for i, row in batch.iterrows()]\n",
    "    \n",
    "    # embedding  \n",
    "    docs = batch['context'].tolist()  # pd.Series to python list\n",
    "#     emb_vectors = [get_embedding(doc, MODEL) for doc in docs] \n",
    "    emb_vectors = embed.embed_documents(docs) # list of list\n",
    "# used the langchain embeddings, can also use the helper func\n",
    "    ids = batch['id'].tolist()\n",
    "    \n",
    "    # upsert\n",
    "    to_upsert = zip(ids, emb_vectors, meta_data)    \n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2526c70a-8b3c-4d24-a519-ba43931da163",
   "metadata": {},
   "source": [
    "# Querying the db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20830213-718e-4036-b708-5835e88e5c89",
   "metadata": {},
   "source": [
    "## Semantic Search type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3bbd484-244c-45cc-862e-defcd90542df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Initialize the vector store with the correct embedding method\n",
    "embeddings = OpenAIEmbeddings(model=MODEL, api_key=OPENAI_API_KEY)\n",
    "\n",
    "\n",
    "vectorstore = PineconeVectorStore(index, embeddings, \"context\", pinecone_api_key= MY_KEY) # df['context'] column is the actual text field to search from\n",
    "\n",
    "# Perform the similarity search, pure semantic, nothing generative\n",
    "query = \"destruction of US fifth fleet\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d357ef-7606-469c-af58-753980682f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#results \n",
    "# Here it is working as semantic search agent which gives the full documents\n",
    "# as context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3389571-7651-4f48-9eca-58d6e9b31cd2",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa9979b2-a68f-4037-a9cc-209227f4f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets use langchain chat models for RAG type answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40d87413-baa3-45c6-ae8d-2a05793cabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory \\\n",
    "import ConversationBufferWindowMemory\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# OpenAI LLM\n",
    "llm = ChatOpenAI(openai_api_key = OPENAI_API_KEY,\n",
    "                model_name = 'gpt-3.5-turbo', # for convenience, can also use other version\n",
    "                temperature = 0.0) # answers from the db only, not generate random answers\n",
    "\n",
    "# conversational memory\n",
    "conv_mem = ConversationBufferWindowMemory(\n",
    "    memory_key = 'chat_history', # can answers based on previous context\n",
    "    k = 5,\n",
    "    return_messages =True)\n",
    "\n",
    "# retrieval qa\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm = llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever = vectorstore.as_retriever())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3874053-1ffa-4d8f-9842-737d14e89a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Which year university of notredame was established',\n",
       " 'result': 'The University of Notre Dame du Lac was established in 1842.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Which year university of notredame was established\"\n",
    "qa.invoke(query) # retrieving the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f816016-3274-46bd-991e-47eb3b74adc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34a15b40-993d-4e0a-a1b7-f1a0fe21286b",
   "metadata": {},
   "source": [
    "# CHATBOT AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "915c0ef4-d543-4ed8-955a-8947b134b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tools to be used by agent\n",
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "    name = 'Knowledge Base',\n",
    "    func = qa.invoke,\n",
    "    description = ('use this when answering based on knowledge')\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2317d98e-37eb-4b3f-b11e-181ff06a8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize agent\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conv_mem \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e386340e-419f-4281-ab98-a05882d06f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent(\"when was university of notredame established\") # chat gpt kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a698c43-aa1c-4ccc-895b-e0193d6b0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent(\"who founded the university\") \n",
    "# will answers about notredame without mentioning using memory_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c76923a7-6650-4809-989e-b1e737d61f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent(\"20+6\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
